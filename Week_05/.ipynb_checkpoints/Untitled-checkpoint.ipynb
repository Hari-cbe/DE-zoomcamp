{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0008b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc77f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/29 01:35:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName(\"v_07\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1da1a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet(\"data/raw/green/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "595659c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green.registerTempTable('green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ef91cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'lpep_pickup_datetime',\n",
       " 'lpep_dropoff_datetime',\n",
       " 'store_and_fwd_flag',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'ehail_fee',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'payment_type',\n",
       " 'trip_type',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d6fbecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        PULocationID as zone,\n",
    "        date_trunc(\"hour\", lpep_pickup_datetime) as hour,\n",
    "        \n",
    "        SUM(total_amount) as ammount,\n",
    "        COUNT(1) as total_number_of_trips\n",
    "        \n",
    "    FROM \n",
    "        green \n",
    "    GROUP BY \n",
    "        1, 2\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f4278d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+------------------+---------------------+\n",
      "|zone|               hour|           ammount|total_number_of_trips|\n",
      "+----+-------------------+------------------+---------------------+\n",
      "| 174|2020-01-01 00:00:00|              21.7|                    1|\n",
      "|  41|2020-01-01 02:00:00|            556.11|                   36|\n",
      "| 173|2020-01-01 05:00:00| 59.03999999999999|                    3|\n",
      "|  75|2020-01-01 07:00:00|127.15999999999998|                   10|\n",
      "|   7|2020-01-01 17:00:00| 357.8400000000001|                   28|\n",
      "|  36|2020-01-01 16:00:00|             18.09|                    2|\n",
      "|  41|2020-01-01 18:00:00| 487.9300000000001|                   33|\n",
      "|  25|2020-01-01 19:00:00|            262.92|                   12|\n",
      "| 179|2020-01-01 23:00:00|            139.97|                    6|\n",
      "|  62|2020-01-02 10:00:00|             75.78|                    6|\n",
      "| 181|2020-01-02 17:00:00|            179.72|                   10|\n",
      "|  83|2020-01-02 18:00:00|             53.89|                    3|\n",
      "|  92|2020-01-02 18:00:00|            160.36|                    8|\n",
      "| 189|2020-01-02 20:00:00|61.010000000000005|                    3|\n",
      "| 243|2020-01-02 22:00:00|26.560000000000002|                    2|\n",
      "| 166|2020-01-03 01:00:00|             20.85|                    2|\n",
      "| 181|2020-01-03 10:00:00|            250.37|                   13|\n",
      "| 213|2020-01-03 16:00:00|              16.8|                    1|\n",
      "|  36|2020-01-03 17:00:00|             99.68|                    4|\n",
      "| 255|2020-01-03 20:00:00|119.36999999999999|                    9|\n",
      "+----+-------------------+------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26fc5272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green_result.write.parquet('data/processed/revenue/green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9153bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read.parquet(\"data/raw/yellow/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cde35ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow.registerTempTable('yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acccbb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge',\n",
       " 'airport_fee']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yellow.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3b38db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        PULocationID as zone,\n",
    "        date_trunc(\"hour\", tpep_pickup_datetime) as hour,\n",
    "        \n",
    "        SUM(total_amount) as ammount,\n",
    "        COUNT(1) as total_number_of_trips\n",
    "        \n",
    "    FROM \n",
    "        yellow \n",
    "    GROUP BY \n",
    "        1, 2\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99ff1658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow_result.write.parquet('data/processed/revenue/yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28799b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green_result = df_green_result.withColumnRenamed(\"ammount\",\"green_ammount\")\\\n",
    "                                 .withColumnRenamed(\"total_number_of_trips\",\"green_trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c06bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow_result = df_yellow_result.withColumnRenamed(\"ammount\",\"yellow_ammount\")\\\n",
    "                                 .withColumnRenamed(\"total_number_of_trips\",\"yellow_trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "533dfe9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[zone: bigint, hour: timestamp, green_ammount: double, green_trips: bigint]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49fd010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joins = df_green_result.join(df_yellow_result,on=['hour','zone'],how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "759c1899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hour: timestamp, zone: bigint, green_ammount: double, green_trips: bigint, yellow_ammount: double, yellow_trips: bigint]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fd59276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-------------+-----------+------------------+------------+\n",
      "|               hour|zone|green_ammount|green_trips|    yellow_ammount|yellow_trips|\n",
      "+-------------------+----+-------------+-----------+------------------+------------+\n",
      "|2020-01-01 04:00:00|   1|         null|       null|              94.8|           1|\n",
      "|2020-01-01 08:00:00|   1|         null|       null|              60.8|           1|\n",
      "|2020-01-01 11:00:00|   1|         null|       null|              90.3|           1|\n",
      "|2020-01-01 12:00:00|   1|         null|       null|266.65999999999997|           3|\n",
      "|2020-01-01 13:00:00|   1|         null|       null|            213.36|           2|\n",
      "|2020-01-01 20:00:00|   1|         null|       null|             96.96|           3|\n",
      "|2020-01-02 15:00:00|   1|         null|       null|246.60000000000002|           2|\n",
      "|2020-01-02 17:00:00|   1|         null|       null|1168.3899999999999|          10|\n",
      "|2020-01-03 05:00:00|   1|       117.39|          1|             100.1|           1|\n",
      "|2020-01-03 06:00:00|   1|         null|       null|            174.36|           1|\n",
      "|2020-01-03 14:00:00|   1|         null|       null|447.39000000000004|           5|\n",
      "|2020-01-03 18:00:00|   1|         null|       null|            226.26|           2|\n",
      "|2020-01-04 15:00:00|   1|         null|       null|460.06000000000006|           6|\n",
      "|2020-01-04 17:00:00|   1|         null|       null|              94.3|           1|\n",
      "|2020-01-06 10:00:00|   1|         null|       null|            397.26|           2|\n",
      "|2020-01-06 11:00:00|   1|         null|       null|            290.85|           2|\n",
      "|2020-01-06 15:00:00|   1|         null|       null|             100.3|           1|\n",
      "|2020-01-08 08:00:00|   1|         null|       null|             95.31|           2|\n",
      "|2020-01-08 11:00:00|   1|         null|       null|            296.74|           2|\n",
      "|2020-01-08 12:00:00|   1|         null|       null|249.85999999999999|           2|\n",
      "+-------------------+----+-------------+-----------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_joins.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33874966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_joins.write.parquet(\"data/processed/revenue/total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4959fcf8",
   "metadata": {},
   "source": [
    "Reading and Joining with the taxi-look-up dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d1fcf572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-29 02:27:40--  https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 3.160.203.184, 3.160.203.53, 3.160.203.81, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|3.160.203.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12331 (12K) [text/csv]\n",
      "Saving to: ‘taxi_zone_lookup.csv.1’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.04K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-02-29 02:27:41 (219 MB/s) - ‘taxi_zone_lookup.csv.1’ saved [12331/12331]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c59d2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.option(\"header\",\"true\").csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba8c7a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21812b0a",
   "metadata": {},
   "source": [
    "Joining Zones and total_revenue \n",
    "\n",
    "Broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d99c28f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a14532d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joins_with_zones = df_joins.join(df_zones, df_joins.zone == df_zones.LocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c96f9fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hour: timestamp, zone: bigint, green_ammount: double, green_trips: bigint, yellow_ammount: double, yellow_trips: bigint, LocationID: string, Borough: string, Zone: string, service_zone: string]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joins_with_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a4262645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hour: timestamp, green_ammount: double, green_trips: bigint, yellow_ammount: double, yellow_trips: bigint, LocationID: string, Borough: string, service_zone: string]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joins_with_zones.drop('zone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b706aa12",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Found duplicate column(s) when inserting into file:/home/haris/zoomcamp-learn/spark_notebooks/data/processed/tmp: `zone`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_joins_with_zones\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/processed/tmp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mparquet(path)\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Found duplicate column(s) when inserting into file:/home/haris/zoomcamp-learn/spark_notebooks/data/processed/tmp: `zone`"
     ]
    }
   ],
   "source": [
    "df_joins_with_zones.write.parquet(\"data/processed/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ebaa9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
